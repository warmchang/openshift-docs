// Module included in the following assemblies:
//
// * scalability_and_performance/planning-your-environment-according-to-object-maximums.adoc

:_mod-docs-content-type: REFERENCE
[id="cluster-maximums-environment_{context}"]
= {product-title} environment and configuration on which the cluster maximums are tested

[role="_abstract"]
To validate your deployment limits, review the environment and configuration details for the cloud platforms on which {product-title} cluster maximums are tested. This reference ensures your infrastructure aligns with the specific scenarios used to validate scalability limits.

[id="aws-cloud-platform-cluster-maximums_{context}"]
== AWS cloud platform cluster maximums

[options="header",cols="8*"]
|===
| Node |Flavor |vCPU |RAM (GiB) |Disk type|Disk size (GiB) or IOS |Count |Region

| Control plane/etcd
| r5.4xlarge
| 16
| 128
| gp3
| 220
| 3
| us-west-2

| Infra
| m5.12xlarge
| 48
| 192
| gp3
| 100
| 3
| us-west-2

| Workload
| m5.4xlarge
| 16
| 64
| gp3
| 500
| 1
| us-west-2

| Compute
| m5.2xlarge
| 8
| 32
| gp3
| 100
| 3/25/250/500
| us-west-2

|===

where:

Control plane/etcd:: Control plane/etcd nodes use gp3 disks with a baseline performance of 3000 IOPS and 125 MiB per second because etcd is latency sensitive. The gp3 volumes do not use burst performance.

Infra:: Infra nodes are used to host Monitoring, Ingress, and Registry components to ensure they have enough resources to run at large scale.

Workload:: The workload node is dedicated to run performance and scalability workload generators.
+ 
Using a larger disk size of 500 GiB ensures that there is enough space to store the large amounts of data that is collected during the performance and scalability test run.

Compute:: The cluster is scaled in iterations of 3, 25, 250, and 500 compute nodes. Performance and scalability tests are executed at the specified node counts.

[id="ibm-power-platform-cluster-maximums_{context}"]
== IBM Power platform cluster maximums

[options="header",cols="6*"]
|===
| Node |vCPU |RAM (GiB) |Disk type |Disk size (GiB) or IOS |Count

| Control plane/etcd
| 16
| 32
| io1
| 120 / 10 IOPS per GiB
| 3

| Infra
| 16
| 64
| gp2
| 120
| 2

| Workload
| 16
| 256
| gp2
| 120
| 1

| Compute
| 16
| 64
| gp2
| 120
| 2 to 100

|===

where:

Control plane/etcd:: io1 disks with 120 / 10 IOPS per GiB are used for control plane/etcd nodes as etcd is I/O intensive and latency sensitive.

Infra:: Infra nodes are used to host Monitoring, Ingress, and Registry components to ensure they have enough resources to run at large scale.

Workload:: Workload node is dedicated to run performance and scalability workload generators.

Workload.120:: Larger disk size is used so that there is enough space to store the large amounts of data that is collected during the performance and scalability test run.

Compute.2 to 100:: Cluster is scaled in iterations.

[id="ibm-z-platform-cluster-maximums_{context}"]
== IBM Z platform cluster maximums

[options="header",cols="6*"]
|===
| Node |vCPU |RAM (GiB) |Disk type |Disk size (GiB) or IOS |Count

| Control plane/etcd
| 8
| 32
| ds8k
| 300 / LCU 1
| 3

| Compute
| 8
| 32
| ds8k
| 150 / LCU 2
| 4 nodes (scaled to 100/250/500 pods per node) 

|===

where:

Control plane/etcd:: Nodes are distributed between two logical control units (LCUs) to optimize disk I/O load of the control plane/etcd nodes as etcd is I/O intensive and latency sensitive. Etcd I/O demand should not interfere with other workloads. Four compute nodes are used for the tests running several iterations with 100/250/500 pods at the same time. First, idling pods were used to evaluate if pods can be instanced. Next, a network and CPU demanding client/server workload were used to evaluate the stability of the system under stress. Client and server pods were pairwise deployed and each pair was spread over two compute nodes.

Compute:: No separate workload node was used. The workload simulates a microservice workload between two compute nodes.

vCPU:: Physical number of processors used is six Integrated Facilities for Linux (IFLs).

RAM (GiB):: Total physical memory used is 512 GiB.

