// Module included in the following assemblies:
//
// * scalability_and_performance/what-huge-pages-do-and-how-they-are-consumed-by-apps.adoc
// * post_installation_configuration/node-tasks.adoc

:_mod-docs-content-type: PROCEDURE
[id="configuring-huge-pages_{context}"]
= Configuring huge pages at boot time

[role="_abstract"]
To ensure nodes in your {product-title} cluster pre-allocate memory for specific workloads, reserve huge pages at boot time. This configuration sets aside memory resources during system startup, offering a distinct alternative to run-time allocation.

There are two ways of reserving huge pages: at boot time and at run time. Reserving at boot time increases the possibility of success because the memory has not yet been significantly fragmented. The Node Tuning Operator currently supports boot-time allocation of huge pages on specific nodes.

ifndef::openshift-origin[]
[NOTE]
====
The TuneD boot-loader plugin only supports {op-system-first} compute nodes.
====
endif::openshift-origin[]

.Procedure

. Label all nodes that need the same huge pages setting by a label by entering the following command:
+
[source,terminal]
----
$ oc label node <node_using_hugepages> node-role.kubernetes.io/worker-hp=
----

. Create a file with the following content and name it `hugepages-tuned-boottime.yaml`:
+
[source,yaml]
----
apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: hugepages
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=Boot time configuration for hugepages
      include=openshift-node
      [bootloader]
      cmdline_openshift_node_hugepages=hugepagesz=2M hugepages=50
    name: openshift-node-hugepages

  recommend:
  - machineConfigLabels:
      machineconfiguration.openshift.io/role: "worker-hp"
    priority: 30
    profile: openshift-node-hugepages
# ...
----
+
where:
+
`metadata.name`:: Specifies the `name` of the Tuned resource to `hugepages`.
`spec.profile`:: Specifies the `profile` section to allocate huge pages.
`spec.profile.data`:: Specifies the order of parameters. The order is important as some platforms support huge pages of various sizes.
`spec.recommend.machineConfigLabels`:: Specifies the enablement of a machine config pool based matching.

. Create the Tuned `hugepages` object by entering the following command:
+
[source,terminal]
----
$ oc create -f hugepages-tuned-boottime.yaml
----

. Create a file with the following content and name it `hugepages-mcp.yaml`:
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-hp
  labels:
    worker-hp: ""
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-hp]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-hp: ""
----

. Create the machine config pool by entering the following command:
+
[source,terminal]
----
$ oc create -f hugepages-mcp.yaml
----

.Verification

* To check that enough non-fragmented memory exists and that all the nodes in the `worker-hp` machine config pool now have 50 2Mi huge pages allocated, enter the following command:
+
[source,terminal]
----
$ oc get node <node_using_hugepages> -o jsonpath="{.status.allocatable.hugepages-2Mi}"
100Mi
----

////
For run-time allocation, kubelet changes are needed, see BZ1819719.
== At run time

.Procedure

. Label the node so that the Node Tuning Operator knows on which node to apply the tuned profile, which describes how many huge pages should be allocated:
+
[source,terminal]
----
$ oc label node <node_using_hugepages> hugepages=true
----

. Create a file with the following content and name it `hugepages-tuned-runtime.yaml`:
+
[source,yaml]
----
apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: hugepages
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=Run time configuration for hugepages
      include=openshift-node
      [vm]
      transparent_hugepages=never
      [sysfs]
      /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages=50
    name: node-hugepages

  recommend:
  - match:
    - label: hugepages
    priority: 30
    profile: node-hugepages
----
+
where:
+
`metadata.name`:: Specifies the `name` of the Tuned resource to `hugepages`.
`spec.profile`:: Specifies the `profile` section to allocate huge pages.
`spec.recommend.match`:: Specifies the `match` section that associates the profile to nodes with the `hugepages` label.

. Create the custom `hugepages` tuned profile by using the `hugepages-tuned-runtime.yaml` file:
+
[source,terminal]
----
$ oc create -f hugepages-tuned-runtime.yaml
----

. After creating the profile, the Operator applies the new profile to the correct
node and allocates huge pages. Check the logs of a tuned pod on a node using
huge pages to verify:
+
[source,terminal]
----
$ oc logs <tuned_pod_on_node_using_hugepages> \
    -n openshift-cluster-node-tuning-operator | grep 'applied$' | tail -n1
----
+
----
2019-08-08 07:20:41,286 INFO     tuned.daemon.daemon: static tuning from profile 'node-hugepages' applied
----
////
